---
title: "Diabetes Risk Predictions"
author: "Seif Kungulio"
output:
  html_document:
    theme: readable
    highlight: espresso
    css: "styles.css"
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width = 10, fig.height = 6
                      )

#Install the following libraries if required
if(!requireNamespace("tidyverse", quietly = TRUE)){
  # If not installed, then install it
  install.packages("tidyverse")
}
if(!requireNamespace("caret", quietly = TRUE)){
  # If not installed, then install it
  install.packages("caret")
}
if(!requireNamespace("GGally", quietly = TRUE)){
  # If not installed, then install it
  install.packages("GGally")
}
if(!requireNamespace("corrplot", quietly = TRUE)){
  # If not installed, then install it
  install.packages("corrplot")
}
if(!requireNamespace("pROC", quietly = TRUE)){
  # If not installed, then install it
  install.packages("pROC")
}
if(!requireNamespace("randomForest", quietly = TRUE)){
  # If not installed, then install it
  install.packages("randomForest")
}
# if(!requireNamespace("janitor", quietly = TRUE)){
#   # If not installed, then install it
#   install.packages("janitor")
# }
if(!requireNamespace("skimr", quietly = TRUE)){
  # If not installed, then install it
  install.packages("skimr")
}
if(!requireNamespace("gt", quietly = TRUE)){
  # If not installed, then install it
  install.packages("gt")
}
if(!requireNamespace("vip", quietly = TRUE)){
  # If not installed, then install it
  install.packages("vip")
}
if(!requireNamespace("tidymodels", quietly = TRUE)){
  # If not installed, then install it
  install.packages("tidymodels")
}
if(!requireNamespace("fastshap", quietly = TRUE)){
  # If not installed, then install it
  install.packages("fastshap")
}
if(!requireNamespace("treemapify", quietly = TRUE)){
  # If not installed, then install it
  install.packages("treemapify")
}
if(!requireNamespace("shapviz", quietly = TRUE)){
  # If not installed, then install it
  install.packages("shapviz")
}
if(!requireNamespace("themis", quietly = TRUE)){
  # If not installed, then install it
  install.packages("themis")
}

# Load the libraries
library(knitr)
library(tidyverse)
library(caret)
library(corrplot)
library(randomForest)
library(ranger)
library(doParallel)
#library(janitor)
library(ggplot2)
library(scales)
library(skimr)
library(gt)
library(GGally)
library(vip)
library(tidymodels)
library(themis)
library(fastshap)
library(shapviz)
library(treemapify)
library(pROC)
```
<hr style="height:5px; border:none; color:#FF0000; background-color:#FF0000;" />

<br>

## Business Understanding
<hr>

### Business Objective
The primary business objective of this project is to support the Texas Department of State Health Services in enhancing diabetes prevention efforts across the state. By leveraging self-reported health behavior and demographic data, the goal is to develop a predictive model by 2026 that identifies Texas adults with a 30% or higher risk of developing diabetes within the next five years. This model aims to facilitate early detection, reduce long-term healthcare costs, promote health equity, and optimize resource allocation in vulnerable communities.

### Problem Statement
How can public health agencies in Texas use self-reported behavioral and demographic data to identify adults at 30%+ risk of developing diabetes within 5 years, using BMI (>30) marker, and implement targeted interventions by 2026?.

### Business Success Criteria
The project will be considered successful if it:

- Produces a validated predictive model with practical thresholds for decision-making
- Demonstrates improved identification of high-risk populations compared to current screening protocols
- Supports targeted outreach that aligns with state health equity goals
- Provides actionable insights for stakeholders through dashboards or visual summaries

<br>

## Data Understanding
<hr/>

### Data Collection
The primary dataset used is the **Diabetes Health Indicators Dataset** sourced from **Kaggle**, based on the **2015 Behavioral Risk Factor Surveillance System (BRFSS)**. This dataset includes over **253,000 observations** and **22 attributes**, offering a rich foundation for predictive modeling using demographic, behavioral, and clinical health indicators.



### Load the data
```{r}
diabetes.df <- read.csv("../resources/diabetes_health_indicators_BRFSS2015.csv")
```
### Check for dimension and structure
```{r}
dim(diabetes.df)
str(diabetes.df)
```
### Check duplicates
Check and remove duplicates if exist
```{r}
diabetes.df <- distinct(diabetes.df)
```
### Check summary
#### Basic summary
```{r}
skim(diabetes.df)
```
#### Statistical summary
```{r}
summary(diabetes.df)
```
### Class balance of target
```{r}
diabetes.df %>% 
  count(Diabetes_binary) %>% 
  mutate(pct = n/sum(n)*100)
```

### Data Description
The dataset contains 253,680 records and 22 variables relevant to diabetes risk prediction. The target variable is binary (Diabetes_binary), while the predictors include behavioral, clinical, and demographic factors such as blood pressure, cholesterol, BMI, physical activity, general health, and income. Most variables (17) are binary categorical, with the remaining 5 being numeric or ordinal. This structure supports classification modeling and offers strong coverage of key health indicators needed to identify individuals at risk for diabetes.

### Data Dictionary

| **Variable** | **Data Type** | **Descriptions**                  | **Constraints** |
|:-------------|:--------------|:----------------------------------|:----------------|
| Diabetes_binary | Number | Diabetes status includes prediabetes. | Values: 0 or 1 |
| HighBP | Number | Ever told you have high blood pressure. | Values: 0 or 1 |
| HighChol | Number | Ever told you have high cholesterol. | Values: 0 or 1 |
| CholCheck | Number | Cholesterol checked within the past 5 years. | Values: 0 or 1 |
| BMI | Number | Body Mass Index (kg/m^2), calculated from self-reported height & weight. | Ranges: ~ 12 to 100 |
| Smoker | Number | Smoked at least 100 cigarettes in lifetime. | Values: 0 or 1 |
| Stroke | Number | Ever told you had a stroke. | Values: 0 or 1 |
| HeartDiseaseorAttack | Number | Ever told you had coronary heart disease (CHD) or myocardial infarction (MI). | Values: 0 or 1 |
| PhysActivity | Number | Any physical activity or exercise in past 30 days, not including job. | Values: 0 or 1 |
| Fruits | Number | Consume fruit 1 or more times per day. | Values: 0 or 1 |
| Veggies | Number | Consume vegetables 1 or more times per day. | Values: 0 or 1 |
| HvyAlcoholConsump | Number | Heavy alcohol consumption (men >14 drinks/week; women >7 drinks/week). | Values: 0 or 1 |
| AnyHealthcare | Number | Have any kind of health care coverage. | Values: 0 or 1 |
| NoDocbcCost | Number | Could not see a doctor in the past 12 months because of cost. | Values: 0 or 1 |
| GenHlth | Number | Self-rated general health (1 = Excellent, 2 = Very good, 3 = Good, 4 = Fair, 5 = Poor). | Ranges: 1 to 5 |
| MentHlth | Number | Number of days mental health not good in past 30 days. | Ranges: 0 to 30 |
| PhysHlth | Number | Number of days physical health not good in past 30 days. | Ranges: 0 to 30 |
| DiffWalk | Number | Serious difficulty walking or climbing stairs. | Values: 0 or 1 |
| Sex | Number | Sex (0 = Female, 1 = Male). | Values: 0 or 1 |
| Age | Number | Age category (1=18 to 24, 2=25 to 29, 3=30 to 34, 4=35 to 39, 5=40 to 44, 6=45 to 49, 7=50 to 54, 8=55 to 59, 9=60 to 64, 10=65 to 69, 11=70 to 74, 12=75 to 79, 13=80+). | Values: 0 or 1 |
| Education | Number | Education level ranges  (1=Never attended/Kindergarten only, 2=Grades 1 to 8, 3=Grades 9 to 11, 4=Grade 12 or GED, 5=Some college or technical school, 6=College 4 years or more). | Ranges: 1 to 6 |
| Income | Number | Household income ranges (1=<\$10,000, 2=\$10,000 to \$15,000, 3=\$15,000 to \$20,000, 4=\$20,000 to \$25,000, 5=\$25,000 to \$35,000, 6=\$35,000 to \$50,000, 7=\$50,000 to \$75,000, 8=\$75,000+).. | Ranges: 1 to 8 |
| | | | |

### Pre-analysis visualization
#### BMI Histogram
```{r}
ggplot(diabetes.df, aes(x = BMI)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "black") +
  theme_test() + labs(title = "Distribution of BMI")
```

#### Age Boxplots
```{r}
ggplot(diabetes.df, aes(x = as.factor(Diabetes_binary), y = Age)) +
  geom_boxplot(fill = "orange") +
  labs(title = "Age vs Diabetes Status", x = "Diabetes", y = "Age Group") +
  theme_test()
```

#### Correlation among numeric variables
```{r}
numeric_vars <- diabetes.df %>% select(BMI, MentHlth, PhysHlth, Age)
corrplot(cor(numeric_vars), method = "color", type = "upper")
```

### Data Quality Assessment
- **Completeness**: No traditional missing values; however, several variables use domain-specific placeholders (e.g., `88`, `77`, `99`) that denote "None", "Don't know", or "Refused".
- **Duplicates**: 24,206 duplicate rows were removed.
- **Validity**: No string-based categorical variables; all attributes are encoded numerically.
- **Outliers**: Variables such as `BMI` have values up to `98`, suggesting the need for outlier treatment or binning.
- **Skewness**: Continuous variables like `BMI`, `MentHlth`, and `PhysHlth` are skewed, potentially impacting model performance.

<br>

## Data Preparation
<hr/>

### Defensive cleaning
Defensive cleaning for **MentHlth** and **PhysHlth** special codes/ placeholders.
In some BRFSS releases, 88 can mean zero **0** days. Hence I will convert 88 to 0 and 77 or 99 to **NA**.
If these codes are not present in the dataset, then the mutate() calls are no-ops.
```{r}
diabetes.df <- diabetes.df |> 
  mutate(
    MentHlth = case_when(MentHlth %in% c(88) ~ 0, 
                         MentHlth %in% c(77,99) ~ NA, 
                         TRUE ~ MentHlth),
    PhysHlth = case_when(PhysHlth %in% c(88) ~ 0, 
                         PhysHlth %in% c(77,99) ~ NA, 
                         TRUE ~ PhysHlth)
  )
```

### Remove NA's
Remove rows containing NA's
```{r}
diabetes.df <- na.omit(diabetes.df)
```

### Remove Outliers
Remove outliers from BMI using IQR method
```{r}
# Calculate IQR bounds for BMI
Q1 <- quantile(diabetes.df$BMI, 0.25, na.rm = TRUE)
Q3 <- quantile(diabetes.df$BMI, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Remove rows with BMI outliers
diabetes.df <- diabetes.df %>%
  filter(BMI >= lower_bound & BMI <= upper_bound)
```

### Binning Mental and Physical Health
```{r}
diabetes.df$MentHlth_binned <- cut(diabetes.df$MentHlth,
                                   breaks = c(-1, 0, 10, 20, 30),
                                   labels = c("None", "Low", "Moderate", "High"))
diabetes.df$PhysHlth_binned <- cut(diabetes.df$PhysHlth,
                                   breaks = c(-1, 0, 10, 20, 30),
                                   labels = c("None", "Low", "Moderate", "High"))
```
The code bins the `MentHlth` and `PhysHlth` variables—representing the number of unhealthy days—into four categorical levels: "None," "Low," "Moderate," and "High." This simplifies analysis, improves interpretability, and supports visualizations or modeling, especially for skewed data. The binning enhances clarity but should be evaluated to ensure meaningful thresholds and balanced group sizes.

### Scale BMI
```{r}
diabetes.df$BMI_scaled <- scale(diabetes.df$BMI)
```

### Age Grouping
```{r}
diabetes.df$Age_group <- cut(diabetes.df$Age,
                             breaks = c(0, 4, 8, 13),
                             labels = c("18–34", "35–54", "55+"))
```
### Feature Engineering:
#### Chronic Risk Load
```{r}
diabetes.df$Chronic_Risk_Load <- with(diabetes.df, HighBP + HighChol + 
                                        Stroke + HeartDiseaseorAttack)
```
The `Chronic_Risk_Load` variable is a newly engineered feature that sums four binary health indicators—high blood pressure, high cholesterol, stroke, and heart disease—to reflect an individual’s total chronic disease burden. Ranging from 0 to 4, it provides a simplified, interpretable measure of health risk that can enhance predictive modeling and population stratification. This feature helps capture the compounded effect of comorbidities and supports deeper analysis of how chronic conditions relate to diabetes and other health outcomes.

#### Healthcare Barrier Index
```{r}
diabetes.df$Healthcare_Barrier_Index <- with(diabetes.df, NoDocbcCost + (1 - AnyHealthcare))
```
The `Healthcare_Barrier_Index` is a new feature that combines lack of insurance and inability to afford doctor visits into a single score ranging from 0 to 2. It captures the extent of healthcare access barriers, with higher values indicating greater difficulty in obtaining care. This index enhances analysis by incorporating structural health risk factors and can be used to examine how access issues relate to health outcomes like diabetes.

### Convert variables to factors
```{r}
diabetes.df$Sex <- factor(ifelse(diabetes.df$Sex == 1, "Male", "Female"))
diabetes.df$GenHlth <- factor(diabetes.df$GenHlth,
                              levels = 1:5,
                              labels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
diabetes.df$Education <- factor(diabetes.df$Education,
                                levels = 1:6,
                                labels = c("<HS", "HS", "Some college", 
                                           "College 1-3y", "College 4y", "Postgrad"))
diabetes.df$Income <- factor(diabetes.df$Income,
                             levels = 1:8,
                             labels = c("<10k", "10-15k", "15-20k", "20-25k",
                                        "25-35k", "35-50k", "50-75k", "75k+"))
```

### Drop unused variables
```{r}
diabetes.df <- diabetes.df %>%
  select(Diabetes_binary, BMI, BMI_scaled, MentHlth_binned, PhysHlth_binned,
         Age_group, Chronic_Risk_Load, Healthcare_Barrier_Index, GenHlth, 
         Education, Income, Smoker, PhysActivity, DiffWalk, 
         HvyAlcoholConsump, Sex)
```

### Export to CSV file
Export to CSV file for further visualization
```{r}
write.csv(diabetes.df, "diabetes_data.csv", row.names = FALSE)
```

### EDA through Visualization

#### Define high-risk group (≥30% probability equivalent proxy)
```{r}
mean_bmi <- mean(diabetes.df$BMI, na.rm = TRUE)  # mean before scaling
sd_bmi <- sd(diabetes.df$BMI, na.rm = TRUE)      # std before scaling
diabetes.df$approx_BMI <- (diabetes.df$BMI_scaled * sd_bmi) + mean_bmi

# Define high-risk flag
diabetes.df$HighRisk <- ifelse(diabetes.df$approx_BMI > 30 &
                                 diabetes.df$Diabetes_binary == 0, 1, 0)
```

#### 1. KPI-style summary plot
```{r}
kpi_data <- diabetes.df %>%
  summarise(
    pct_high_risk = mean(HighRisk) * 100,
    total_high_risk = sum(HighRisk),
    avg_BMI_high_risk = mean(approx_BMI[HighRisk == 1]),
    avg_chronic_load = mean(Chronic_Risk_Load[HighRisk == 1])
  )
kable(kpi_data)
```

#### 2. Risk by Age & Sex
```{r}
ggplot(diabetes.df, aes(x = Age_group, fill = Sex)) +
  geom_bar(position = "fill") +
  facet_wrap(~HighRisk) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "% High Risk by Age and Sex",
    x = "Age Group", y = "Percentage",
    fill = "Sex"
  ) +
  theme_test()
```

#### 3. Socio-economic Treemap (Education + Income)
```{r}
df_treemap <- diabetes.df %>%
  group_by(Education, Income) %>%
  summarise(
    count = n(),
    pct_high_risk = mean(HighRisk) * 100
  )

ggplot(df_treemap, aes(area = count, fill = pct_high_risk,
                       label = paste(Education, "\n", Income))) +
  geom_treemap() +
  geom_treemap_text(colour = "white", place = "centre", grow = TRUE) +
  scale_fill_gradient(low = "lightblue", high = "red") +
  labs(title = "High-Risk Prevalence by Education & Income",
       fill = "% High Risk") +
  theme_test()
```

#### 4. Behavioral & Health Markers
```{r}
df_long <- diabetes.df %>%
  select(HighRisk, Smoker, PhysActivity, DiffWalk, HvyAlcoholConsump) %>%
  pivot_longer(cols = -HighRisk, names_to = "Behavior", values_to = "Value")

ggplot(df_long, aes(x = Behavior, fill = factor(Value))) +
  geom_bar(position = "fill") +
  facet_wrap(~HighRisk) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Behavioral Factors by Risk Group",
    x = "Behavioral Factor", y = "Percentage",
    fill = "Response"
  ) +
  theme_test()
```

#### 5. General Health vs Risk
```{r}
ggplot(diabetes.df, aes(x = GenHlth, fill = factor(HighRisk))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(
    title = "General Health Ratings vs Risk Status",
    x = "General Health", y = "Percentage",
    fill = "High Risk"
  ) +
  theme_test()
```

### Convert categorical variables to factor
```{r}
diabetes.df <- diabetes.df %>%
  mutate(across(where(is.character), as.factor),
         across(c(Diabetes_binary, GenHlth, Education, Income, Smoker, PhysActivity,
                  DiffWalk, HvyAlcoholConsump, Sex, Age_group,
                  MentHlth_binned, PhysHlth_binned), as.factor))
```

### Train/Test split (stratified)

```{r}
set.seed(123)
train_idx <- createDataPartition(diabetes.df$Diabetes_binary, 
                                 p = 0.8, list = FALSE)
train <- diabetes.df[train_idx, ]
test <- diabetes.df[-train_idx, ]
```

<br>

## Modeling
<hr/>

### Logistic Regression
This is initial baseline model.

#### Fit logistic model
```{r}
model_logit <- glm(Diabetes_binary ~ ., data = train, family = binomial)
```

#### Model Summary
```{r}
summary(model_logit)
```
#### Predict probabilities and classes
```{r}
test_probs <- predict(model_logit, newdata = test, type = "response")
test_pred <- ifelse(test_probs > 0.5, 1, 0)
```
#### Evaluate logistic model
```{r}
confusionMatrix(as.factor(test_pred), test$Diabetes_binary, positive = "1")
```
#### Logistic ROC curve
```{r}
roc_obj <- roc(response = test$Diabetes_binary, predictor = test_probs, quiet = TRUE)

plot(roc_obj,
     main = "ROC Curve - Logistic Regression",
     col = "blue",
     lwd = 2,
     print.auc = TRUE,
     print.auc.pattern = "AUC = %.3f",
     print.auc.cex = 1.1,
     print.auc.x = 0.65, print.auc.y = 0.2)
abline(a = 0, b = 1, lty = 2, col = "red")

```

```{r include=FALSE}
#knitr::knit_exit()
```

### Random Forest
Random Forest was selected as alternative model to implement with tuning focus.

```{r}
# Ensure target is a clean 2-level factor (positive = "Yes")
train$Diabetes_binary <- factor(ifelse(train$Diabetes_binary == 1, "Yes", "No"),
                                levels = c("No","Yes"))
test$Diabetes_binary  <- factor(ifelse(test$Diabetes_binary  == 1, "Yes", "No"),
                                levels = c("No","Yes"))

# Remove near-zero-variance predictors (speeds split finding)
nzv <- nearZeroVar(train[, setdiff(names(train), "Diabetes_binary")], names = TRUE)
keep <- setdiff(names(train), nzv)
train2 <- train[, c("Diabetes_binary", setdiff(keep, "Diabetes_binary"))]
test2  <- test[,  c("Diabetes_binary", setdiff(keep, "Diabetes_binary"))]

# 5-fold CV setup (ROC)
cv5 <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final",
  allowParallel = TRUE
)

# Parallelize CV
n_cores <- max(1, parallel::detectCores() - 1)
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl)

# Small, informed grid around sqrt(p)
p <- ncol(train2) - 1
grid <- expand.grid(
  mtry = unique(pmax(1L, round(c(sqrt(p), p/5, p/3)))),
  splitrule = "gini",
  min.node.size = c(2L, 5L, 10L)
)

set.seed(123)
rf_cv <- train(
  Diabetes_binary ~ .,
  data = train2,
  method = "ranger",          # << fast
  metric = "ROC",
  trControl = cv5,
  tuneGrid = grid,            # << small, sensible grid
  num.trees = 300,            # << fewer trees than 500; usually enough
  importance = "impurity",
  probability = TRUE,
  oob.error = TRUE
)

stopCluster(cl)
registerDoSEQ()

print(rf_cv)
plot(rf_cv)

# ---- Evaluate on held-out test ----
rf_probs <- predict(rf_cv, newdata = test2, type = "prob")[, "Yes"]
rf_preds <- factor(ifelse(rf_probs >= 0.5, "Yes", "No"), levels = c("No","Yes"))

confusionMatrix(rf_preds, test2$Diabetes_binary, positive = "Yes")

rf_roc_obj <- roc(response = test2$Diabetes_binary, predictor = rf_probs, quiet = TRUE)
plot(rf_roc_obj,
     main = "ROC Curve - Ranger RF (5-fold CV)",
     col = "blue", lwd = 2,
     print.auc = TRUE,
     print.auc.pattern = "AUC = %.3f",
     print.auc.cex = 1.1,
     print.auc.x = 0.65, print.auc.y = 0.2)
abline(a = 0, b = 1, lty = 2, col = "red")
```


```{r include=FALSE}
knitr::knit_exit()
```


#### Model Camparison
```{r}
# Logistic Regression Metrics
logit_cm <- confusionMatrix(as.factor(test_pred), test$Diabetes_binary, positive = "1")
logit_auc <- auc(roc_obj)

logit_metrics <- data.frame(
  Model = "Logistic Regression",
  Accuracy = logit_cm$overall["Accuracy"],
  Precision = logit_cm$byClass["Precision"],
  Recall = logit_cm$byClass["Recall"],
  F1 = logit_cm$byClass["F1"],
  AUC = as.numeric(logit_auc)
)

# Random Forest Metrics
rf_cm <- confusionMatrix(rf_preds, test$Diabetes_binary, positive = "1")
rf_auc <- auc(rf_roc_obj)

rf_metrics <- data.frame(
  Model = "Random Forest",
  Accuracy = rf_cm$overall["Accuracy"],
  Precision = rf_cm$byClass["Precision"],
  Recall = rf_cm$byClass["Recall"],
  F1 = rf_cm$byClass["F1"],
  AUC = as.numeric(rf_auc)
)

# Combine and Display
comparison_table <- bind_rows(logit_metrics, rf_metrics)

kable(comparison_table, caption = "Model Comparison: Logistic Regression vs Random Forest", digits = 3)
```

```{r include=FALSE}
knitr::knit_exit()
```






### Random Forest
Random Forest was selected as alternative model to implement with tuning focus.

#### Train Random Forest Model
```{r}
rf_model <- randomForest(Diabetes_binary ~ ., 
                         data = train, ntree = 500)
```

#### Make predictions on random forest model
```{r}
rf_preds <- predict(rf_model, newdata = test)
```

#### Evaluate random forest model
```{r}
confusionMatrix(rf_preds, test$Diabetes_binary)
```
#### Random Forest ROC Curve
```{r}
rf_probs <- predict(rf_model, newdata = test, type = "prob")[, 2]
rf_roc_obj <- roc(test$Diabetes_binary, rf_probs)

plot(rf_roc_obj,
  main = "ROC Curve - Random Forest",
  col = "blue",
  lwd = 2,
  print.auc = TRUE,
  print.auc.pattern = "AUC = %.3f",
  print.auc.cex = 1.1,
  print.auc.x = 0.65, print.auc.y = 0.2)
abline(a = 0, b = 1, lty = 2, col = "red")
```

#### Model Camparison
```{r}
# Logistic Regression Metrics
logit_cm <- confusionMatrix(as.factor(test_pred), test$Diabetes_binary, positive = "1")
logit_auc <- auc(roc_obj)

logit_metrics <- data.frame(
  Model = "Logistic Regression",
  Accuracy = logit_cm$overall["Accuracy"],
  Precision = logit_cm$byClass["Precision"],
  Recall = logit_cm$byClass["Recall"],
  F1 = logit_cm$byClass["F1"],
  AUC = as.numeric(logit_auc)
)

# Random Forest Metrics
rf_cm <- confusionMatrix(rf_preds, test$Diabetes_binary, positive = "1")
rf_auc <- auc(rf_roc_obj)

rf_metrics <- data.frame(
  Model = "Random Forest",
  Accuracy = rf_cm$overall["Accuracy"],
  Precision = rf_cm$byClass["Precision"],
  Recall = rf_cm$byClass["Recall"],
  F1 = rf_cm$byClass["F1"],
  AUC = as.numeric(rf_auc)
)

# Combine and Display
comparison_table <- bind_rows(logit_metrics, rf_metrics)

kable(comparison_table, caption = "Model Comparison: Logistic Regression vs Random Forest", digits = 3)
```


```{r include=FALSE}
knitr::knit_exit()
```




<br>

## Evaluation & Reporting
<hr/>




<br>

## Interpretation (SHAP)
<hr/>




<br>

## Deployment
<hr/>



### Random Forest
Random Forest was selected as alternative model to implement with tuning focus.

```{r}
set.seed(123)

# Ensure target is a 2-level factor with valid names avoiding "0" or "1"
# Treat "1" as the positive class ("Yes")
train$Diabetes_binary <- factor(ifelse(train$Diabetes_binary == 1, "Yes", "No"),
                                levels = c("No","Yes"))
test$Diabetes_binary  <- factor(ifelse(test$Diabetes_binary  == 1, "Yes", "No"),
                                levels = c("No","Yes"))

# 5-fold CV setup
cv5 <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,  # enables ROC, Sens, Spec
  savePredictions = "final"
)

# Train Random Forest with CV (optimize by ROC)
rf_cv <- train(
  Diabetes_binary ~ .,
  data = train,
  method = "rf",
  metric = "ROC",
  trControl = cv5,
  tuneLength = 7,
  ntree = 300,
  importance = TRUE
)

rf_cv
plot(rf_cv)

# Evaluate on the held-out test set
rf_probs <- predict(rf_cv, newdata = test, type = "prob")[, "Yes"]
rf_preds <- factor(ifelse(rf_probs >= 0.5, "Yes", "No"), 
                   levels = c("No","Yes"))

confusionMatrix(rf_preds, test$Diabetes_binary, positive = "Yes")

# ROC Curve on test
rf_roc_obj <- roc(response = test$Diabetes_binary, 
                  predictor = rf_probs, quiet = TRUE)

plot(rf_roc_obj,
     main = "ROC Curve - Random Forest (5-fold CV model)",
     col = "blue",
     lwd = 2,
     print.auc = TRUE,
     print.auc.pattern = "AUC = %.3f",
     print.auc.cex = 1.1,
     print.auc.x = 0.65, print.auc.y = 0.2)
abline(a = 0, b = 1, lty = 2, col = "red")

```
